# LLM_Research_Review
# This repo is dedicated to all LLM related papers


## Foundational Papers on LLM Architecture and Pretraining

| LLM Paper                 | Status  | Reading | 
| ------------------------  |---------| --------| 
|[Attention is All You Need](https://arxiv.org/abs/1706.03762)|:white_check_mark: ||
|[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)| :white_check_mark: ||
|[Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)|:white_check_mark: ||
|[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)|:white_check_mark: ||


GPT
| LLM Paper                 | Status  | Reading | 
| ------------------------  |---------| --------| 
|[Improving Language Understanding by Generative Pre-Training(GPT-1)](https://arxiv.org/abs/2108.07258)|:white_check_mark: ||
|[Language Models are Unsupervised Multitask Learners(GPT-2)](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|:white_check_mark: ||
|[Language Models are Few-Shot Learners(GPT-3)](https://arxiv.org/abs/2005.14165)|:white_check_mark: ||


## Methods for Improving LLM Efficiency
| LLM Paper                 | Status  | Reading | 
| ------------------------  |---------| --------| 
|[FlashAttention: A Scalable Framework for Efficient Attention Mechanisms](https://arxiv.org/abs/2205.14135)|:white_check_mark: ||
|[Cramming: Efficient Training of Large-Scale Models without Layerwise Pretraining](https://arxiv.org/abs/2212.14034)|:white_check_mark: ||


## Methods for Controlling LLM Outputs
| LLM Paper                 | Status  | Reading | 
| ------------------------  |---------| --------| 
|[InstructGPT: Controllable Text Generation with Content-Planning Transformer](https://arxiv.org/abs/2203.02155)|:white_check_mark: ||
|[Constitutional AI: Aligning Language Models with Human Values](https://arxiv.org/abs/2212.08073)|:white_check_mark: ||


## Alternative (ChatGPT) LLM Architectures
| LLM Paper                 | Status  | Reading | 
| ------------------------  |---------| --------| 
|[BLOOM: A Distributed Open-Source Implementation of LLMs](https://arxiv.org/abs/2211.05100)|:white_check_mark: ||
|[Sparrow: A Large-Scale Language Model for Conversational AI](https://arxiv.org/abs/2209.14375)|:white_check_mark: ||
|[BlenderBot 3: Recipes for Building Large-Scale Conversational Agents](https://arxiv.org/abs/2208.03188)|:white_check_mark: ||



Important Ethical Concerns Regarding LLMs
| LLM Paper                 | Status  | Reading | 
| ------------------------  |---------| --------| 
|[On the Opportunities and Risks of Foundation Models](https://arxiv.org/abs/2108.07258)|:white_check_mark: ||
|[GPT-3: Its Nature, Scope, Limits, and Consequences](https://link.springer.com/article/10.1007/s11023-020-09548-1)|:white_check_mark: ||
|[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/abs/10.1145/3442188.3445922)|:white_check_mark: ||


